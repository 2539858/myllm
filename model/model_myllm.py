# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MyLLM Config
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

from transformers import PretrainedConfig


class Config(PretrainedConfig):
    model_type = "MyLLM"

    def __init__(
            self,
            dropout: float = 0.0,
            bos_token_id: int = 1,
            eos_token_id: int = 2,
            hidden_act: str = 'silu',
            hidden_size: int = 512,
            intermediate_size: int = None,
            max_position_embeddings: int = 32768,
            num_attention_heads: int = 8,
            num_hidden_layers: int = 8,
            num_key_value_heads: int = 2,
            vocab_size: int = 6400,
            rms_norm_eps: float = 1e-05,
            rope_theta: int = 1000000.0,
            inference_rope_scaling: bool = False,
            flash_attn: bool = True,
            **kwargs
    ):
        super().__init__(**kwargs)
        self.dropout = dropout
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id
        self.hidden_act = hidden_act
        self.hidden_size = hidden_size
        self.intermediate_size = intermediate_size
        self.max_position_embeddings = max_position_embeddings
        self.num_attention_heads = num_attention_heads
        self.num_hidden_layers = num_hidden_layers
        self.num_key_value_heads = num_key_value_heads
        self.vocab_size = vocab_size
        self.rms_norm_eps = rms_norm_eps
        self.rope_theta = rope_theta
        self.inference_rope_scaling = inference_rope_scaling
        # å¤–æ¨é•¿åº¦ = factor * original_max_position_embeddings = 32768
        self.rope_scaling = {
            "beta_fast": 32,
            "beta_slow": 1,
            "factor": 16,
            "original_max_position_embeddings": 2048,
            "attention_factor": 1.0,
            "type": "yarn"
        } if self.inference_rope_scaling else None
        self.flash_attn = flash_attn



# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜
#                                             MyLLM Model
# ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜ğŸ“˜

import math
import torch
import torch.nn.init as init
import torch.nn.functional as F
from torch import nn
from transformers.activations import ACT2FN
from typing import Optional, Tuple, List, Union
from transformers import PreTrainedModel, GenerationMixin, PretrainedConfig
from transformers.modeling_outputs import CausalLMOutputWithPast


class RMSNorm(torch.nn.Module):
    def __init__(self, dim: int, eps: float = 1e-5):
        super().__init__()
        # é˜²æ­¢åˆ†æ¯ä¸º0
        self.eps = eps
        # å¯å­¦ä¹ å‚æ•° Î³ï¼Œ å½’ä¸€åŒ–åï¼Œç»™æ¯ä¸ªç»´åº¦ä¸€ä¸ªå¯è°ƒçš„ç¼©æ”¾
        self.weight = nn.Parameter(torch.ones(dim))

    def _norm(self, x):
        # .pow(2)å¯¹æ¯ä¸ªå…ƒç´ æ±‚å¹³æ–¹   .mean(-1, keepdim=True) åœ¨æœ€åä¸€ä¸ªç»´åº¦æ±‚å‡å€¼  rsqrtç›¸å½“äº1 / torch.sqrt(a)
        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)

    def forward(self, x):
        # float()è®©æ•°å€¼æ›´ç¨³å®šï¼Œé˜²æ­¢RMSç²¾åº¦ä¸å¤Ÿ  å†è½¬å›åŸæœ¬xçš„ç±»å‹
        return self.weight * self._norm(x.float()).type_as(x)

# ç”Ÿæˆæ‰€æœ‰ä½ç½®ç¼–ç æ‰€éœ€çš„é¢‘ç‡å’Œä¸‰è§’å‡½æ•°çŸ©é˜µ
# dimï¼šæ¯ä¸ªattention headçš„ç»´åº¦ï¼›endï¼šæœ€å¤§ä½ç½®é•¿åº¦ï¼›rope_baseï¼šæ§åˆ¶ä¸åŒç»´åº¦æ—‹è½¬é¢‘ç‡çš„è¡°å‡é€Ÿåº¦, rope_scalingï¼šé•¿ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥
def precompute_freqs_cis(dim: int, end: int = int(32 * 1024), rope_base: float = 1e6,
                         rope_scaling: Optional[dict] = None):
    # torch,arrange(0, dim, 2) å–[0, 2, Â·Â·Â·, dim-2], :dim//2è¡¨ç¤ºæˆ‘ä»¬åªè¦å‰ dim//2ä¸ªå…ƒç´ ï¼Œå½“dimä¸ºå¥‡æ•°çš„æ—¶å€™æœ‰ç”¨
    # .float è½¬æ¢æˆfloat32ï¼Œæ–¹ä¾¿è¿›è¡Œé™¤æ³•æ“ä½œ æœ€ç»ˆå¾—åˆ° 1/ base^(2i/d)
    freqs  = 1.0 / (rope_base ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    attn_factor = 1.0
    # rope_scaling ä¸ºè¶…é•¿çš„ä¸Šä¸‹æ–‡å‡†å¤‡
    if rope_scaling is not None:
        # åˆ†åˆ«æ‹¿åˆ°ï¼Œæœ€å¤§ä¸Šä¸‹æ–‡é•¿åº¦ã€ç¼©æ”¾å› å­ã€å¿«Betaå‚æ•°ã€æ…¢Betaå‚æ•°
        orig_max, factor, beta_fast, beta_slow, attn_factor = (
            rope_scaling.get("original_max_position_embeddings", 2048), rope_scaling.get("factor", 16),
            rope_scaling.get("beta_fast", 32.0), rope_scaling.get("beta_slow", 1.0), rope_scaling.get("attention_factor", 1.0)
        )
        # å¦‚æœè¶…å‡ºå¤§å°ã€‚è¿›è¡Œå¤æ‚ç¼©æ”¾ ä½¿ç”¨YaRN çš„â€œé¢‘ç‡å¹³æ»‘ç¼©æ”¾â€  ç°åœ¨ç”¨çš„æœ€å¤§é•¿åº¦ > æ¨¡å‹åŸå§‹è®­ç»ƒé•¿åº¦ï¼Œåªå‹ä½â€œé«˜é¢‘ç»´åº¦â€çš„æ—‹è½¬é€Ÿåº¦ï¼Œä¿ç•™ä½é¢‘ç»´åº¦çš„é•¿ç¨‹å»ºæ¨¡èƒ½åŠ›
        if end / orig_max > 1.0:
            # åå‘æ˜ å°„å‡½æ•°/ ä¸¤ä¸ªæ–¹ç¨‹å¾—åˆ°içš„å€¼ bæ˜¯è¾“å…¥çš„å€¼
            inv_dim = lambda b: (dim * math.log(orig_max / (b * 2 * math.pi))) / (2 * math.log(rope_base))
            # [0, low)å®Œå…¨ä¸ç¼©æ”¾ (low, high) ä»0-1çº¿æ€§ç¼©æ”¾ï¼Œ (high, end)å®Œå…¨ç¼©æ”¾
            # ç¡®å®šä»å“ªä¸€ç»´åº¦å¼€å§‹è¿›è¡Œå¹³æ»‘ç¼©æ”¾ï¼Œè¾“å…¥é˜ˆå€¼beta_fastï¼Œè®¡ç®—ç»´åº¦ç´¢å¼•iï¼Œfloorå–ä¸è¶…è¿‡è¯¥å€¼çš„æœ€å°æ•´æ•°ï¼Œmax(Â·Â·Â·ï¼Œ0)ä¿è¯ä¸‹ç•Œä¸å°äº0
            low = max(math.floor(inv_dim(beta_fast)), 0)
            # ç¡®å®šå¹³æ»‘ç¼©æ”¾ç»“æŸçš„ç»´åº¦ç´¢å¼•ï¼Œè¾“å…¥é˜ˆå€¼beta_low,è®¡ç®—ç»´åº¦ç´¢å¼•iï¼Œceilå‘ä¸Šå–æ•´ï¼Œmin(Â·Â·Â·,dim//2 - 1)ä¿è¯ä¸Šç•Œä¸è¶…è¿‡å¯ç”¨ç»´åº¦èŒƒå›´
            high = min(math.ceil(inv_dim(beta_slow)), dim // 2 - 1)
            # æ„é€ çº¿æ€§æ–œå¡ï¼Œç®—å‡º Î³ å› å­å¯¹äºæ¯ä¸€ä¸ª RoPE ç»´åº¦ç´¢å¼• i çš„å€¼ã€‚
            # torch.arange(dim // 2, device=freqs.device).float()ç”Ÿæˆ[0,2Â·Â·Â·,dim//2-1]ï¼Œ
            # -lowè®©å¹³æ»‘è¿‡æ¸¡åŒºèµ·ç‚¹ä¸º0 é™¤ä»¥high-lowï¼Œå¾—åˆ°0-1çš„æ¯”ä¾‹Î³ï¼Œmaxæ˜¯ä¸ºäº†é˜²æ­¢é™¤ä»¥0
            # torch.clamp(Â·Â·Â·,0,1)æŠŠÎ³é™åˆ¶åœ¨(0,1)ï¼Œi < low â†’ Î³ = 0,i > high â†’ Î³ = 1,low â‰¤ i â‰¤ high â†’ Î³ çº¿æ€§å¢åŠ 
            ramp = torch.clamp((torch.arange(dim // 2, device=freqs.device).float() - low) / max(high - low, 0.001), 0, 1)
            # YaRN: f'(i) = f(i)((1-Î³) + Î³/s), where Î³âˆˆ[0,1] is linear ramp
            freqs = freqs * (1 - ramp + ramp / factor)
    # ç”Ÿæˆ[0,1,2Â·Â·Â·,end-1]åºåˆ—è¡¨ç¤ºtokençš„ç´¢å¼•t
    t = torch.arange(end, device=freqs.device)
    # ç¬¬iè¡Œç¬¬jåˆ—è¿›è¡Œï¼Œa_i * b_j çš„å¤–ç§¯æ“ä½œ
    freqs = torch.outer(t, freqs).float()
    # RoPE æ˜¯â€œäºŒç»´æ—‹è½¬â€ï¼Œæ¯ä¸¤ä¸ª hidden ç»´åº¦å…±äº«åŒä¸€ä¸ªé¢‘ç‡ã€‚ (x_2i, x_2i+1)
    # freq shape [end, dim//2], Q/Kç»´åº¦[end, dim] ç”¨torch.cat([cos,cos],dim=-1)æ¥å®ç°ï¼Œdim=-1åœ¨æœ€åä¸€ä¸ªç»´åº¦æ‹¼æ¥
    # attn_factor å¯¹ cos/sinåšä¸€ä¸ªæ•´ä½“çš„ç¼©æ”¾
    freqs_cos = torch.cat([torch.cos(freqs), torch.cos(freqs)], dim=-1) * attn_factor
    freqs_sin = torch.cat([torch.sin(freqs), torch.sin(freqs)], dim=-1) * attn_factor
    return freqs_cos, freqs_sin

# å°†ä½ç½®ä¿¡æ¯çœŸæ­£æ³¨å…¥ Query å’Œ Key å‘é‡
def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):
    def rotate_half(x):
        # ååŠéƒ¨åˆ†å–è´Ÿå¹¶ç§»åˆ°å‰é¢ï¼Œå‰åŠéƒ¨åˆ†ä¸å˜ç§»åˆ°åé¢
        # x.shape[-1] // 2: ä¿ç•™ååŠéƒ¨åˆ†ï¼Œ-xæ¯ä¸ªå…ƒç´ å–è´Ÿï¼Œå¹¶æ”¾åˆ°å‰åŠéƒ¨åˆ†ã€‚ æœ€ååœ¨æœ€åä¸€ç»´è¿›è¡Œæ‹¼æ¥
        return torch.cat((-x[..., x.shape[-1] // 2:], x[..., : x.shape[-1] // 2]), dim=-1)
    # åœ¨ç¬¬ä¸€ç»´æ’å…¥ä¸€ä¸ªç»´åº¦ï¼Œç”¨æ¥å¯¹é½headçš„ç»´åº¦
    q_embed = (q * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(q) * sin.unsqueeze(unsqueeze_dim))
    k_embed = (k * cos.unsqueeze(unsqueeze_dim)) + (rotate_half(k) * sin.unsqueeze(unsqueeze_dim))
    return q_embed, k_embed


# æŠŠkey/value å¼ é‡é‡Œçš„æ¯ä¸ªæ³¨æ„åŠ›å¤´ï¼ˆheadï¼‰æŒ‰ n_rep æ¬¡æ•°é‡å¤å¤åˆ¶ï¼Œç”Ÿæˆæ›´å¤šçš„ key/value headï¼Œä»¥ä¾¿ä¸ query head å¯¹é½æˆ–è¿›è¡Œå¹¿æ’­è®¡ç®—
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    # batch_sizeä¸€æ¬¡è¾“å…¥æœ‰å¤šå°‘ä¸ªæ ·æœ¬, sequence_lengthåºåˆ—é•¿åº¦ï¼Œä¹Ÿå°±æ˜¯æ¯ä¸ªæ ·æœ¬çš„ token æ•°é‡
    # num_key_value_heads key/value æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œhead_dimæ¯ä¸ªå¤´çš„ç»´åº¦,æ¯ä¸ª head å†…éƒ¨çš„å‘é‡é•¿åº¦
    bs, slen, num_key_value_heads, head_dim = x.shape
    # ä¸éœ€è¦å¤åˆ¶ï¼Œç›´æ¥è¿”å›x
    if n_rep == 1:
        return x
    # æŠŠ key/value å¼ é‡çš„æ³¨æ„åŠ›å¤´æŒ‰ n_rep æ¬¡æ•°å¤åˆ¶
    # x shape [bs, slen, num_key_value_heads, head_dim] expandæ‰©å±•æˆ [bs, slen, num_key_value_heads, n_rep, head_dim]
    # å†reshapeæˆ [bs, slen, num_key_value_heads * n_rep, head_dim]
    return (
        x[:, :, :, None, :].expand(bs, slen, num_key_value_heads, n_rep, head_dim).reshape(bs, slen, num_key_value_heads * n_rep, head_dim)
    )

# GQAã€RoPEã€KVcache
class Attention(nn.Module):
    def __init__(self, args: Config):
        super().__init__()
        # å¦‚æœæ²¡æœ‰æŒ‡å®šKVå¤´æ•°ï¼Œå°±é»˜è®¤å’Œquery headä¸€æ ·
        self.num_key_value_heads = args.num_attention_heads if args.num_key_value_heads is None else args.num_key_value_heads
        # ä¿è¯ query head èƒ½æ•´é™¤ key/value headï¼Œè¿™æ ·æ¯ä¸ª key/value head å¯ä»¥è¢«å‡ ä¸ª query head å…±äº«
        assert args.num_attention_heads % self.num_key_value_heads == 0
        self.n_local_heads = args.num_attention_heads # attention headæ•°
        self.n_local_kv_heads = self.num_key_value_heads # kv headæ•°
        self.n_rep = self.n_local_heads // self.n_local_kv_heads # K/Vå¤´è¢«å‡ ä¸ªQå¤´å…±äº«
        self.head_dim = args.hidden_size // args.num_attention_heads # æ¯ä¸ª attention head çš„ç»´åº¦
        # åˆ†åˆ«ç”¨çº¿æ€§å±‚ç”Ÿæˆ Qã€Kã€Vï¼ŒQé’ˆå¯¹æ‰€æœ‰çš„query headï¼ŒK/Vé’ˆå¯¹å°‘é‡key/value head
        self.q_proj = nn.Linear(args.hidden_size, args.num_attention_heads * self.head_dim, bias=False)
        self.k_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        self.v_proj = nn.Linear(args.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)
        # æœ€åå°†å¤šå¤´æ³¨æ„åŠ›çš„è¾“å‡ºåˆå¹¶å›åŸæ¥çš„éšè—ç»´åº¦ï¼Œè¿™é‡Œç”¨çš„æ˜¯query head
        self.o_proj = nn.Linear(args.num_attention_heads * self.head_dim, args.hidden_size, bias=False)
        self.attn_dropout = nn.Dropout(args.dropout) # åˆ›å»ºç”¨äºæ³¨æ„åŠ›æƒé‡çš„ dropout å±‚ã€‚
        self.resid_dropout = nn.Dropout(args.dropout) # åˆ›å»ºç”¨äºæ®‹å·®è¿æ¥ä¹‹åçš„ dropout å±‚
        self.dropout = args.dropout # å­˜åˆ°å®ä¾‹self.dropout
        # åˆ¤æ–­Flashæ˜¯å¦å¯ç”¨ï¼Œï¼ˆpytorch>=2.0æ”¯æŒé«˜æ•ˆæ³¨æ„åŠ›ï¼‰
        # torch.nn.functional.scaled_dot_product_attentionï¼Œå®ƒä¼šè‡ªåŠ¨é€‰æ‹©æœ€é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°
        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention') and args.flash_attn
        # print("WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0")

    def forward(self,
                x: torch.Tensor,
                # æ¥æ”¶æ—‹è½¬ä½ç½®ç¼–ç 
                position_embeddings: Tuple[torch.Tensor, torch.Tensor],  # ä¿®æ”¹ä¸ºæ¥æ”¶coså’Œsin
                # KVç¼“å­˜
                past_key_value: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,
                use_cache=False,
                attention_mask: Optional[torch.Tensor] = None):
        bsz, seq_len, _ = x.shape # x.shape [batch_size, seq_len, hidden_size]
        # xq (bsz, seq_len, num_query_heads * head_dim) -> (bsz, seq_len, n_local_heads, head_dim)
        # xk (bsz, seq_len, num_kv_heads * head_dim) -> (bsz, seq_len, n_local_kv_heads, head_dim)
        # xv (bsz, seq_len, num_kv_heads * head_dim) -> (bsz, seq_len, n_local_kv_heads, head_dim)
        xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)
        # æ‹†æˆå¤šå¤´
        xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)
        # ä»position_embeddingsä¸­æ¥æ”¶coså’Œsin
        cos, sin = position_embeddings
        # ä½ç½®ä¿¡æ¯æ³¨å…¥QKå‘é‡    æ¨¡å‹æ”¯æŒçš„æœ€å¤§åºåˆ—é•¿åº¦æ˜¯ max_seq_lenï¼Œä½†å½“å‰è¾“å…¥çš„å®é™…åºåˆ—é•¿åº¦æ˜¯ seq_lenï¼Œå¯èƒ½å°äºæœ€å¤§é•¿åº¦ï¼›
        # æ‰€ä»¥åªéœ€è¦å–å‰ seq_len ä¸ªä½ç½®å¯¹åº”çš„ cos å’Œ sin å€¼ï¼Œä¸å½“å‰ batch çš„ xqã€xk å¯¹é½
        xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])

        # kv_cacheå®ç°
        if past_key_value is not None:
            # æŠŠä¹‹å‰çš„kv valueåœ¨dim=1ä¸Šè¿›è¡Œæ‹¼æ¥
            xk = torch.cat([past_key_value[0], xk], dim=1)
            xv = torch.cat([past_key_value[1], xv], dim=1)
        past_kv = (xk, xv) if use_cache else None # å¦‚æœuse_cache=Trueï¼Œåˆ™ä¿å­˜å½“å‰æ—¶é—´æ­¥çš„keyå’Œvalueç»„æˆæ–°ç¼“å­˜past_kv

        xq, xk, xv = (
            # KVå¤åˆ¶åˆ°å’ŒQå¤´æ•°ç›¸åŒï¼Œæœ€ç»ˆQKVç»´åº¦å®Œå…¨ç›¸åŒã€‚ 1ã€2ç»´åº¦äº¤æ¢ï¼Œqkvç»´åº¦ç»´åº¦è°ƒæ•´ä¸º(batch_size, num_heads, seq_len, head_dim)
            xq.transpose(1, 2),
            repeat_kv(xk, self.n_rep).transpose(1, 2),
            repeat_kv(xv, self.n_rep).transpose(1, 2)
        )
        # å¦‚æœ PyTorch æ”¯æŒ flash attentionï¼Œå¹¶ä¸”åºåˆ—é•¿åº¦å¤§äº 1ï¼Œåˆ™ç”¨é«˜æ•ˆå®ç°ï¼ŒFlash Attentionä¸æ”¯æŒå¤æ‚mask
        if self.flash and seq_len > 1 and (attention_mask is None or torch.all(attention_mask == 1)):
            # é«˜æ•ˆattentionæ“ä½œï¼Œåªæœ‰è®­ç»ƒæ—¶å¯ç”¨dropout_pï¼Œis_causal=Trueè‡ªåŠ¨åŠ å› æœmask
            output = F.scaled_dot_product_attention(xq, xk, xv, dropout_p=self.dropout if self.training else 0.0, is_causal=True)
        else:
            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim) # QK^T / sqrt(d)
            scores = scores + torch.triu(
                # torch.triu(Â·Â·Â·,diagonal=1)ä¸Šä¸‰è§’å…¨éƒ¨æ˜¯-infï¼Œè¡¨ç¤ºtokenä¸èƒ½çœ‹æœªæ¥ scores+mask å±è”½ä¸Šä¸‰è§’çš„å€¼
                torch.full((seq_len, seq_len), float("-inf"), device=scores.device),
                diagonal=1
            ).unsqueeze(0).unsqueeze(0)  # è¿”å›çš„æ˜¯ä¸€ä¸ªäºŒç»´çŸ©é˜µï¼Œæ‰€ä»¥éœ€è¦å‡ç»´ä¸¤æ¬¡

            if attention_mask is not None:
                # attention_mask shape (bsz, seq_len) -> (bsz, 1, 1, seq_len)
                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)
                # æœ‰æ•ˆä½ç½®ä»1å˜æˆ0ï¼Œæ— æ•ˆä½ç½®ä»0å˜æˆ-10^9
                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9
                scores = scores + extended_attention_mask # å’Œæ— æ•ˆä½ç½®ç›¸åŠ åå˜æˆ-inf(-10^9å¯ä»¥å½“ä½œè´Ÿæ— ç©·)

            scores = F.softmax(scores.float(), dim=-1).type_as(xq) # å¾—åˆ°æ³¨æ„åŠ›æƒé‡åè½¬å›åŸdtype
            scores = self.attn_dropout(scores) # æ­£åˆ™åŒ–
            output = scores @ xv # æœ€åä¹˜ä¸ŠV
        # å°†æœ€åä¸¤ä¸ªç»´åº¦åˆå¹¶å›æ¥
        # (bsz, n_heads, seq_len, head_dim) -> (bsz, seq_len, n_heads, head_dim) -> (bsz, seq_len, n_heads * head_dim)
        output = output.transpose(1, 2).reshape(bsz, seq_len, -1)
        output = self.resid_dropout(self.o_proj(output)) # è¾“å‡ºæŠ•å½±ï¼Œå†ä½¿ç”¨æ®‹å·®é“¾æ¥ï¼Œå¾—åˆ°output
        return output, past_kv

# SwiGLUå‰é¦ˆç¥ç»å±‚
class FeedForward(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        if config.intermediate_size is None:
            intermediate_size = int(config.hidden_size * 8 / 3) # 8/3æ˜¯SwiGLUç»éªŒå…¬å¼ï¼ŒLLaMAé‡Œé¢å°±è¿™æ ·ç”¨
            # å¯¹é½åˆ°64ï¼Œæé«˜çŸ©é˜µä¹˜æ³•çš„æ•ˆç‡ï¼Œæ˜¯å·¥ä¸šçº§æ¨¡å‹çš„æ ‡å‡†æ“ä½œ
            config.intermediate_size = 64 * ((intermediate_size + 64 - 1) // 64)
        # gateå†³å®šå“ªäº›ç»´åº¦è¯¥é€šè¿‡ï¼Œupæä¾›çœŸæ­£çš„ä¿¡æ¯å†…å®¹
        self.gate_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False) # ç”Ÿæˆé—¨æ§ä¿¡å·ï¼Œä½¿ç”¨æ¿€æ´»å‡½æ•°
        self.up_proj = nn.Linear(config.hidden_size, config.intermediate_size, bias=False)  # ç”Ÿæˆè¢«è°ƒåˆ¶çš„ä¿¡æ¯ï¼Œä¸åŠ æ¿€æ´»ï¼Œä¿ç•™çº¿æ€§è¡¨è¾¾èƒ½åŠ›
        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False) # å‹å›hidden_size
        self.dropout = nn.Dropout(config.dropout)
        self.act_fn = ACT2FN[config.hidden_act] # æ ¹æ®config.hidden_actå†³å®šä½¿ç”¨ä»€ä¹ˆæ¿€æ´»å‡½æ•°ï¼Œè¿™é‡Œå‰é¢å®šä¹‰äº†æ˜¯silu

    def forward(self, x):
        # é—¨æ§ç›¸ä¹˜ gateã€up size (bsz, seq_len, intermediate_size)
        # down size (bs, seq_len, intermediate_size) -> (bs, seq_len, hidden_size)
        return self.dropout(self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x)))


# ä¸€ä¸ª Block = è‡ªæ³¨æ„åŠ› + å‰é¦ˆç½‘ç»œ + ä¸¤æ¬¡ RMSNorm + æ®‹å·®è¿æ¥
class Block(nn.Module):
    def __init__(self, layer_id: int, config: Config):
        super().__init__()
        self.num_attention_heads = config.num_attention_heads # å¤šå¤´æ³¨æ„åŠ›çš„å¤´æ•°
        self.hidden_size = config.hidden_size # æ€»ç©ºé—´ç»´åº¦
        self.head_dim = config.hidden_size // config.num_attention_heads
        self.self_attn = Attention(config) # GQA + RoPE + KV cache

        # åœ¨Attentionå‰å’ŒMLPå‰ï¼Œå„æ”¾ä¸€ä¸ªRMSNormã€‚å¯¹æ¯ä¸ªtokençš„hidden_sizeç»´å‘é‡ï¼ŒåšRMSNormï¼Œæœ€åepsä¸ºäº†é˜²æ­¢åˆ†æ¯ä¸º0è®¾ç½®çš„ç¨³å®šé¡¹
        self.input_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.post_attention_layernorm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        self.mlp = FeedForward(config) # SwiGLUå‰é¦ˆç½‘ç»œ

    # hidden_states (bsz, seq_len, hidden_size);position_embeddings (cos, sin);past_key_value KV cache æ¨ç†æ—¶ç”¨
    def forward(self, hidden_states, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):
        residual = hidden_states # ä¿å­˜è¾“å…¥ï¼Œç”¨äºæ®‹å·®é“¾æ¥
        # 1.å…ˆ RMSNorm 2.æŠŠå½’ä¸€åŒ–åçš„ç»“æœé€å…¥ Attention 3. Attention è¾“å‡ºæ–°çš„ hidden_statesï¼Œæ›´æ–°åçš„ KV cache
        hidden_states, present_key_value = self.self_attn(
            self.input_layernorm(hidden_states), position_embeddings,
            past_key_value, use_cache, attention_mask
        )
        hidden_states += residual # ç¬¬ä¸€æ¡æ®‹å·® x <- x + Attention(RMSNorm(x)),æŠŠæ³¨æ„åŠ›å±‚å­¦åˆ°çš„æ–°ä¿¡æ¯åŠ åˆ°æ—§ä¿¡æ¯ä¸Š
        # ç¬¬äºŒæ¡æ®‹å·®FFN 1.å†åšä¸€æ¬¡ RMSNorm 2.é€å…¥ SwiGLU FFN 3.è¾“å‡ºå’Œè¾“å…¥å†åšä¸€æ¬¡æ®‹å·®ç›¸åŠ  x <- x + FFN(RMSNorm(x))
        hidden_states = hidden_states + self.mlp(self.post_attention_layernorm(hidden_states)) # ä¿®æ­£æ•°æ®åˆ†å¸ƒ
        return hidden_states, present_key_value # hidden_stateæœ¬å±‚è¾“å‡ºï¼Œpresent_key_valueä¾›ä¸‹ä¸€æ­¥ç”Ÿæˆä½¿ç”¨


class Model(nn.Module):
    def __init__(self, config: Config):
        super().__init__()
        self.config = config
        self.vocab_size = config.vocab_size # è¯è¡¨å¤§å°
        self.num_hidden_layers = config.num_hidden_layers # æ¨¡å‹æœ‰å¤šå°‘å±‚
        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size) # æŠŠç¦»æ•£token_id å˜æˆè¿ç»­è¯­ä¹‰å‘é‡
        self.dropout = nn.Dropout(config.dropout) # é˜²æ­¢è¿‡æ‹Ÿåˆ
        # Block Ã— num_hidden_layersï¼Œä¸€ä¸ªBlockå°±æ˜¯ä¸€ä¸ªå®Œæ•´çš„transformerå±‚ï¼Œè¿™ä¸ªæ“ä½œæŠŠå„ä¸ªå±‚ç»™é€ å‡ºæ¥
        self.layers = nn.ModuleList([Block(l, config) for l in range(self.num_hidden_layers)])
        self.norm = RMSNorm(config.hidden_size, eps=config.rms_norm_eps)
        # dimï¼šæ¯ä¸ªattention headçš„ç»´åº¦ï¼›endï¼šæœ€å¤§ä½ç½®é•¿åº¦ï¼›rope_baseï¼šæ§åˆ¶ä¸åŒç»´åº¦æ—‹è½¬é¢‘ç‡çš„è¡°å‡é€Ÿåº¦, rope_scalingï¼šé•¿ä¸Šä¸‹æ–‡æ‰©å±•ç­–ç•¥
        freqs_cos, freqs_sin = precompute_freqs_cis(dim=config.hidden_size // config.num_attention_heads,
                                                    end=config.max_position_embeddings, rope_base=config.rope_theta,
                                                    rope_scaling=config.rope_scaling)
        # RoPEçš„coså’Œsinæ³¨å†Œæˆâ€œéå‚æ•°å¼ é‡ç¼“å†²åŒºâ€ï¼Œå®ƒä»¬ä¼šéšæ¨¡å‹ä¸€èµ·to.(device)ä½†ä¸å‚ä¸è®­ç»ƒï¼Œé»˜è®¤ä¸ä¿å­˜è¿›checkpoint
        # ï¼ˆå£°æ˜è¿™æ˜¯å‚æ•°çš„ä¸€éƒ¨åˆ†ï¼Œä½†ä¸æ˜¯å‚æ•°ï¼‰
        self.register_buffer("freqs_cos", freqs_cos, persistent=False)
        self.register_buffer("freqs_sin", freqs_sin, persistent=False)

    # Decoder-only Transformerçš„å®Œæ•´å‰å‘ä¼ æ’­é€»è¾‘
    # input_ids:è¾“å…¥çš„token_id; past_value-keys:æ¯ä¸€å±‚ä¿å­˜çš„KV cacheï¼Œç”¨äºä¸‹ä¸€æ­¥å¢é‡æ¨ç†ï¼›use_cache:æ˜¯å¦è¿”å›presentç”¨äºä¸‹ä¸€æ­¥å¢é‡æ¨ç†
    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                **kwargs):
        batch_size, seq_length = input_ids.shape
        # å…ˆåˆ¤æ–­ past_key_values æ˜¯å¦æ˜¯ HuggingFace æ—§æ¥å£ï¼ˆæœ‰ .layers å±æ€§ï¼‰ï¼Œå¦‚æœæ˜¯å°±ä¸¢æ‰
        if hasattr(past_key_values, 'layers'): past_key_values = None
        past_key_values = past_key_values or [None] * len(self.layers) # å¦‚æœä¸ºç©ºï¼Œåˆ™åˆå§‹åŒ–ä¸º[None] * num_layers
        # åˆå§‹ä½ç½®ï¼Œå¦‚æœæ˜¯å¢é‡æ¨ç†(past_key_values é None),å·²ç»ç”Ÿæˆäº†past_lenä¸ªtokenï¼Œæ–°tokençš„positionä»past_lenå¼€å§‹,å¦åˆ™ä»0å¼€å§‹
        start_pos = past_key_values[0][0].shape[1] if past_key_values[0] is not None else 0
        hidden_states = self.dropout(self.embed_tokens(input_ids)) # æŠŠtoken_id -> hidden_vector (batch,seq,hidden_size)

        # ç”Ÿæˆå½“å‰ä½ç½®çš„RoPE embedding
        position_embeddings = (
            self.freqs_cos[start_pos:start_pos + seq_length],
            self.freqs_sin[start_pos:start_pos + seq_length]
        )

        presents = []
        for layer_idx, (layer, past_key_value) in enumerate(zip(self.layers, past_key_values)):
            hidden_states, present = layer(
                hidden_states,
                position_embeddings,
                past_key_value=past_key_value,
                use_cache=use_cache,
                attention_mask=attention_mask
            )
            presents.append(present)

        hidden_states = self.norm(hidden_states)


        return hidden_states, presents


class ForCausalLM(PreTrainedModel, GenerationMixin):
    config_class = Config

    def __init__(self, config: Config = None):
        self.config = config or Config()
        super().__init__(self.config)
        self.model = Model(self.config)
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)
        self.model.embed_tokens.weight = self.lm_head.weight

    def forward(self,
                input_ids: Optional[torch.Tensor] = None,
                attention_mask: Optional[torch.Tensor] = None,
                past_key_values: Optional[List[Tuple[torch.Tensor, torch.Tensor]]] = None,
                use_cache: bool = False,
                logits_to_keep: Union[int, torch.Tensor] = 0,
                **args):
        hidden_states, past_key_values = self.model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            past_key_values=past_key_values,
            use_cache=use_cache,
            **args
        )
        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep
        logits = self.lm_head(hidden_states[:, slice_indices, :])
        output = CausalLMOutputWithPast(logits=logits, past_key_values=past_key_values, hidden_states=hidden_states)
        return output
